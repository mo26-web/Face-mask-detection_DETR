{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detr_Maskdetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "metadata": {
        "id": "CNubzFHIfKNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/maskdata\n"
      ],
      "metadata": {
        "id": "DlTWqdZggRmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/maskdata\""
      ],
      "metadata": {
        "id": "ai74bti3gSU2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d andrewmvd/face-mask-detection"
      ],
      "metadata": {
        "id": "bhle2BA5gUsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gdrive/MyDrive/maskdata/face-mask-detection.zip"
      ],
      "metadata": {
        "id": "UF_oHMIdgXwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/detr.git\n"
      ],
      "metadata": {
        "id": "K4xcuOY_fbLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install albumentations==0.4.6\n"
      ],
      "metadata": {
        "id": "N94gAX09iHGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/maskdata/detr')\n",
        "\n",
        "from detr.models.matcher import HungarianMatcher\n",
        "from detr.models.detr import SetCriterion\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.text as text\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "e9l3F8vSgjIy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "num_classes = 4\n",
        "num_queries = 20\n",
        "null_class_coef = 0.2\n",
        "BATCH_SIZE = 32\n",
        "LR = 2e-5\n",
        "EPOCHS = 200\n",
        "ANNOT_DIR = '/content/gdrive/MyDrive/maskdata/annotations'\n",
        "device = torch.device('cuda')\n",
        "\n",
        "def get_objects(xml_file):\n",
        "    annotation = ET.parse(xml_file)\n",
        "    root = annotation.getroot()\n",
        "    \n",
        "    name = os.path.basename(xml_file).replace('.xml','')\n",
        "    size = root.find('size')\n",
        "    \n",
        "    objects = []\n",
        "    for obj in root.findall('object'):\n",
        "        bbox = obj.find('bndbox')\n",
        "        new_object = {}\n",
        "        new_object['image_id'] = name\n",
        "        new_object['labels'] = obj.find('name').text\n",
        "        new_object['width'] = int(size.find('width').text)\n",
        "        new_object['height'] = int(size.find('height').text)\n",
        "        new_object['x'] = int(bbox.find('xmin').text)\n",
        "        new_object['y'] = int(bbox.find('ymin').text)\n",
        "        new_object['w'] = int(bbox.find('xmax').text)-int(bbox.find('xmin').text)\n",
        "        new_object['h'] = int(bbox.find('ymax').text)-int(bbox.find('ymin').text)\n",
        "        objects.append(new_object)\n",
        "    return objects\n"
      ],
      "metadata": {
        "id": "xj3kOhMFhTQt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "seed_everything(seed)"
      ],
      "metadata": {
        "id": "4-r4d6CXhd_L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annots = []\n",
        "for xml in os.listdir(ANNOT_DIR):\n",
        "    annots += get_objects(os.path.join(ANNOT_DIR,xml))\n",
        "df = pd.DataFrame(annots)\n",
        "\n",
        "# Adjust w, h\n",
        "for i in df.index:\n",
        "    exceed_w =  df.iloc[i].x + df.iloc[i].w - df.iloc[i].width\n",
        "    exceed_h =  df.iloc[i].y + df.iloc[i].h - df.iloc[i].height\n",
        "    if exceed_w > 0:\n",
        "        df.loc[df.index == i,'w'] -= exceed_w\n",
        "    if exceed_h > 0:\n",
        "        df.loc[df.index == i,'h'] -= exceed_h\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df.labels = encoder.fit_transform(df.labels)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2_VMZWWfhe97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = sns.countplot(x='labels',data=df)\n",
        "g.set_xticklabels(encoder.classes_);"
      ],
      "metadata": {
        "id": "9nAO7ukbhtIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_split = df[['image_id']].copy()\n",
        "# df_split = pd.concat([df_split,pd.get_dummies(df.labels).astype('int32')],axis=1)\n",
        "df_split['bbox_count'] = 1\n",
        "df_split = df_split.groupby('image_id').sum()\n",
        "\n",
        "g = plt.figure(figsize=(15,4))\n",
        "g = sns.countplot(x='bbox_count',data=df_split)"
      ],
      "metadata": {
        "id": "t75xBGSehu0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = df_split[:int(len(df_split)*.9)]\n",
        "val_split = df_split[int(len(df_split)*.9):]"
      ],
      "metadata": {
        "id": "gJsSKL0lhwHG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,2,figsize=(13,4))\n",
        "sns.countplot(x='labels',data=df[df['image_id'].isin(train_split.index)],ax=axs[0])\n",
        "axs[0].set_title('Labels distribution in training set')\n",
        "sns.countplot(x='labels',data=df[df['image_id'].isin(val_split.index)],ax=axs[1])\n",
        "axs[1].set_title('Labels distribution in validation set')"
      ],
      "metadata": {
        "id": "RpvAp_gehyge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transform():\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(),\n",
        "        A.RandomRotate90(),\n",
        "        A.RandomBrightnessContrast(),\n",
        "        A.Resize(300, 300),\n",
        "        ToTensorV2(),\n",
        "    ], bbox_params=A.BboxParams(format='coco',label_fields=['labels']))\n",
        "\n",
        "def valid_trainsform():\n",
        "    return A.Compose([\n",
        "        A.Resize(300,300),\n",
        "        ToTensorV2(),\n",
        "    ], bbox_params=A.BboxParams(format='coco',label_fields=['labels']))"
      ],
      "metadata": {
        "id": "T6mqgKR6h1a_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_DIR = '/content/gdrive/MyDrive/maskdata/images'\n",
        "\n",
        "class MaskDataset(Dataset):\n",
        "    def __init__(self, image_ids, dataframe, transforms=None):\n",
        "        self.image_ids = image_ids\n",
        "        self.df = dataframe\n",
        "        self.transforms = transforms\n",
        "    def __len__(self) -> int:\n",
        "        return self.image_ids.shape[0]\n",
        "    def __getitem__(self, index):\n",
        "        image_id = self.image_ids[index]\n",
        "        records = self.df[self.df['image_id']==image_id]\n",
        "        \n",
        "        image = cv2.imread(f'{IMG_DIR}/{image_id}.png', cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        \n",
        "        boxes = records[['x', 'y', 'w', 'h']].values\n",
        "        area = boxes[:,2]*boxes[:,3]\n",
        "        area = torch.as_tensor(area, dtype=torch.float32)\n",
        "        \n",
        "        labels =  records['labels'].values\n",
        "        \n",
        "        if self.transforms:\n",
        "            sample = {\n",
        "                'image': image,\n",
        "                'bboxes': boxes,\n",
        "                'labels': labels\n",
        "            }\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "            boxes = sample['bboxes']\n",
        "            labels = sample['labels']\n",
        "            \n",
        "        _, h, w = image.shape\n",
        "        boxes = A.augmentations.bbox_utils.normalize_bboxes(sample['bboxes'],rows=h,cols=w)  \n",
        "        \n",
        "        target = {}\n",
        "        target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n",
        "        target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n",
        "        target['image_id'] = torch.tensor([index])\n",
        "        target['area'] = area\n",
        "        \n",
        "        return image, target, image_id"
      ],
      "metadata": {
        "id": "VXBS84MGh2Am"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "kLh3EzUkh-MS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MaskDataset(image_ids=train_split.index.values,\n",
        "                           dataframe=df,\n",
        "                           transforms=train_transform())\n",
        "train_data_loader = DataLoader(train_dataset,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             num_workers=2,\n",
        "                             collate_fn=collate_fn)\n",
        "val_dataset = MaskDataset(image_ids=val_split.index.values,\n",
        "                           dataframe=df,\n",
        "                           transforms=valid_trainsform())\n",
        "val_data_loader = DataLoader(val_dataset,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             num_workers=2,\n",
        "                             collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "rTis9Hjth-kC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = HungarianMatcher()\n",
        "weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n",
        "losses = ['labels', 'boxes', 'cardinality']"
      ],
      "metadata": {
        "id": "byLQIUSciATl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "7Pxnrg1XifBi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DETRModel(nn.Module):\n",
        "    def __init__(self, num_classes, num_queries):\n",
        "        super(DETRModel, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_queries = num_queries\n",
        "        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "        self.in_features = self.model.class_embed.in_features\n",
        "        self.model.class_embed = nn.Linear(in_features=self.in_features, out_features=self.num_classes)\n",
        "        self.model.num_queries = self.num_queries\n",
        "    def forward(self, images):\n",
        "        return self.model(images)"
      ],
      "metadata": {
        "id": "q3SvKE4Piglq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "import torch\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "import matplotlib.patches as patches\n"
      ],
      "metadata": {
        "id": "PWxIHwj8ir37"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(data_loader, model, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    \n",
        "    summary_loss = AverageMeter()\n",
        "    \n",
        "    for step, (images, targets, image_ids) in enumerate(data_loader):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "        output = model(images)\n",
        "        loss_dict = criterion(output, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "        \n",
        "        losses = sum(loss_dict[k]*weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        summary_loss.update(losses.item(), BATCH_SIZE)\n",
        "    return summary_loss\n",
        "\n",
        "def eval_fn(data_loader, model, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    criterion.eval()\n",
        "    \n",
        "    summary_loss = AverageMeter()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for step, (images, targets, image_ids) in enumerate(data_loader):\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            \n",
        "            output = model(images)\n",
        "            loss_dict = criterion(output, targets)\n",
        "            weight_dict = criterion.weight_dict\n",
        "            losses = sum(loss_dict[k]*weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "            \n",
        "            summary_loss.update(losses.item(), BATCH_SIZE)\n",
        "    return summary_loss"
      ],
      "metadata": {
        "id": "6ApKVCjfit37"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['WANDB_CONSOLE'] = 'off'\n",
        "\n",
        "def run():\n",
        "    model = DETRModel(num_classes=num_classes, num_queries=num_queries)\n",
        "    #model = get_model_instance_segmentation(3)\n",
        "    model = model.to(device)\n",
        "    criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef=null_class_coef, losses=losses)\n",
        "    criterion = criterion.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    best_loss = 10**5\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        train_loss = train_fn(train_data_loader, model, criterion, optimizer, device, epoch=epoch)\n",
        "        valid_loss = eval_fn(val_data_loader, model, criterion, device, epoch)\n",
        "        if valid_loss.avg < best_loss:\n",
        "            best_loss = valid_loss.avg\n",
        "            torch.save(model.state_dict(), f'detr_best.pth')\n",
        "        if epoch%10 == 0:\n",
        "            print(f'Epoch {epoch+0:03}: | Train Loss: {train_loss.avg:.5f} | Val Loss: {valid_loss.avg:.5f}')"
      ],
      "metadata": {
        "id": "pWt0C4kzjUoj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = run()\n"
      ],
      "metadata": {
        "id": "p7Rm2WysjYbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample(index):\n",
        "    images, targets, image_ids = next(iter(val_data_loader))\n",
        "    images = list(img.to(device) for img in images)\n",
        "    image = images[index].permute(1,2,0).cpu().numpy()\n",
        "    original_image = image.copy()\n",
        "\n",
        "    boxes = targets[index]['boxes'].numpy()\n",
        "    boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,image.shape[0],image.shape[1])]\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "    pred_classes = outputs['pred_logits'][index].softmax(1).cpu().numpy()\n",
        "    pred_boxes = outputs['pred_boxes'][index].cpu().numpy()\n",
        "    pred_boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(pred_boxes,image.shape[0],image.shape[1])]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    for box in boxes:\n",
        "        cv2.rectangle(image,\n",
        "                      (box[0], box[1]),\n",
        "                      (box[2]+box[0], box[3]+box[1]),\n",
        "                      (255, 0, 0), 1)\n",
        "    class_color = {0:(255, 255, 0),1:(0, 0, 255),2:(0, 255, 0)}\n",
        "    for pred_class, pred_box in zip(pred_classes, pred_boxes):\n",
        "        c = pred_class.argmax(0)\n",
        "        if c != 3:\n",
        "            pred_prob = pred_class.max(0)\n",
        "            cv2.rectangle(image,\n",
        "                  (pred_box[0], pred_box[1]),\n",
        "                  (pred_box[2]+pred_box[0], pred_box[3]+pred_box[1]),\n",
        "                  class_color[c], 1)\n",
        "            cv2.rectangle(image,\n",
        "                         (pred_box[0]+33,pred_box[1]),\n",
        "                         (pred_box[0], pred_box[1]-15),\n",
        "                          class_color[c], -1)\n",
        "            ax2.text(pred_box[0], pred_box[1]-8, f'Class:{c}',color='w',fontsize=9)\n",
        "            ax2.text(pred_box[0], pred_box[1], f'Prob:{int(pred_prob*100)}%',color='w',fontsize=9)\n",
        "    ax1.set_axis_off()\n",
        "    ax1.imshow(original_image)\n",
        "    ax2.set_axis_off()\n",
        "    ax2.imshow(image)"
      ],
      "metadata": {
        "id": "rIWzQ2dhsjyP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cpu')"
      ],
      "metadata": {
        "id": "1lCyOYao82WU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DETRModel(num_classes=num_classes,num_queries=num_queries)\n",
        "model.load_state_dict(torch.load(\"./detr_best.pth\", map_location=device))\n",
        "show_sample(14)"
      ],
      "metadata": {
        "id": "6wvscQm9snbm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}